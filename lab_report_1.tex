\documentclass{report}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{mathtools}
\renewcommand{\familydefault}{\sfdefault}
\begin{document}
% ------------------------------------------ %
%                 START HERE                 %
% ------------------------------------------ %
\title{CS5785 Homework 1\\
\large{
    Applied Machine Learning
}
}


\author{
	Vijay Pillai\\
	Thomas Matecki}
\maketitle
\section*{Programming Exercises}

\begin{enumerate}
	\item Digit Recognizer
	\begin{enumerate}[label=(\alph*)]
		\item 
		Training and test data are located in files \textit{train.csv} and \textit{test.csv} respectively. The training data comprises 42000 images and the test data comprises 28000 images.

		\item Figure 1 is one of each digit(0-9) displayed as a 28 by 28 pixel grid.  
		\begin{center}
		\captionof{figure}{}
		\includegraphics[width=12cm]{images/one_grid_x_each_digit.png}
		\end{center}
		\item The histogram below illustrates the the distribution of digits within the training data. It is s not-uniform, but comparable. 
		\begin{center}
		\includegraphics[width=12cm]{images/distrib.png}
		\end{center}
		
		\newpage
		
		\item 10 randomly picked digits and their closest matches are below.
		
		\begin{center}
		\includegraphics[width=12cm]{images/matches.png}
		\end{center}
		
		\item Below are the histograms for the genuine and impostor distances on the same
set of axes.\\
\begin{center}
		\includegraphics[width=12cm]{images/binary_comp.png}
		\end{center}
		
		\newpage
		
		\item The image below exhibits the ROC curve for the genuine and imposter matches. The equal error rate is displayed at $\approx .78$. In the case of binary comparison, the error rate of a classifier that simply guesses is $\frac{1}{2}$.
		
		\begin{center}
		\includegraphics[width=12cm]{images/ROC.png}
		\end{center}
		 
		 
		\item The KNN classifier is implmented in file \textit{mnist/digit\_recognizer.py} as function \textit{knn}
		\item The KNN classifier is implmented in file \textit{mnist/digit\_recognizer.py} as function \textit{cross\_validate}
		\item The confusion matrix for all digits is displayed below, with the vertical axis representing \textit{actual} values and the horizontal axis representing \textit{predicted}.\\
		\begin{center}
		\includegraphics[width=12cm]{images/confusion_matrix.png}
		\end{center}
		\item See function \textit{kaggle\_knn} in \textit{mnist/digit\_recognizer.py}. The output csv was submitted to kaggle with the following result:
		\begin{center}
		\includegraphics[width=16cm]{images/mnist_kaggle.png}
		\end{center}
	\end{enumerate}
	
	\newpage
	
	\item The Titanic Disaster
	\begin{enumerate}[label=(\alph*)]
		\item The test datasets can be found in the \textit{./titanic/titanic\_data} directory.
		\item Implementation is completed using scipy's \textit{LogisticRegression} class and can be found in file \textit{./titanic/titanic.py}. Two things to note about the implementation:
		\begin{itemize}
		\item The categoricaly features are translated to \textit{One Hot} encoding before the classifier is parsed.
		\item Features \textit{PassengerId, Name, Ticket Fare, Cabin}, and \textit{Embarked} are ommitted from the classifer.  Through some trial and error, one can draw the conclusion that omitting these features yields a more accurate classifier, most likely because they are scarcely populated(E.g. \textit{Cabin} has values in the test set do not exist in the training data) or contain a a high number of unique, categorical values(E.g. \textit{Fare}). The values thus increase the dimensionality of the model while offering nothing but idiosyncracies that likely exist only in the sample training data.
		\end{itemize}
		\item the result of the classifier have been submitted to Kaggle and can be found in the \textit{./titanic/titanic\_data}. 
		\begin{center}
		\includegraphics[width=16cm]{images/titanic_kaggle.png}
		\end{center}
	\end{enumerate}
	
\end{enumerate}

\section*{Written Exercises}	
\begin{enumerate}
	\item Based on the defintions of variance and covariance,:\\
	\centerline{$var(X) = E[(X - E[X])^2]$}
	\centerline{$cov(X) = E[(Y - E[Y])(X - E[X])$}
	we have:
	\begin{align*}
    var[X-Y] &= E[( X - Y - E[X - Y])^2]  \\
    	 	 &= E[( X - Y - E[X] - E[Y])^2] \\
    	 	 &= E[( X - E[X]) - (Y - E[Y]) )^2] \\
    	 	 &= E[( X - E[X])^2 - 2E[(Y - E[Y])(X - E[X])] + E[(Y - E[Y])^2]] \\
    	 	 &= var(X) + var(Y) - 2E[(Y - E[Y])(X - E[X])] \\
    	 	 &= var(X) + var(Y) - 2cov(X,Y ) \\
    \end{align*}
	\item Letting event \textit{R} be a positive test result, and event \textit{D} be the occurence of a defective widget, we have...
	\begin{align*}
	     & P(D) = \frac{1}{100,000} \\
	     & P(R \vert D) = .95  \\
	     & P(\lnot R \vert \lnot D) = .95 \\
	 \end{align*}
	    ...and therefore...
	    \begin{align*}
	     & P(\lnot D) 				= 1 - \frac{1}{100,000} = \frac{99,999}{100,000} \\
	     & P(\lnot R \vert 		D) 	= 1-  P(R \vert D) =.05  & 
	     \text{(because $R \cap \lnot R = \emptyset$)}\\
	     & P(R 		\vert \lnot D) 	= .05  \\
	 	\end{align*}
	 	Additionally, the probability of a positive test result is the sum of the probabilities of a  defective widget testing positive and a non-defective widget testing positive; i.e.,
	 	\begin{align*}
	 	 P(R) 	&= P(R \cap D) + P(R \cap \lnot D)\\
	 	 		&= P(R \vert D)P(D) + P(R \vert \lnot D)P(\lnot D) \\
	 	 		&= (.95)\frac{1}{100,000} + (.05)\frac{99,999}{100,000} 
	 	 		 = \frac{.95}{100,000} + \frac{4999.95}{100,000} 
	 	 		 = \mathbf{\frac{5000.9}{100,000}}\\
	    \end{align*}
	    
	     
	    \begin{enumerate}[label=(\alph*)]
		\item Given a positive test result, the probability the widget is actually defective is:
	 	\begin{align*}
	 	 P(D \vert R)	&= \frac{P(R \vert D)P(D)}{P(R)} & \text{(bayes rule)} \\
	 	 				&= \frac{\frac{.95}{100,000}}{\frac{5000.9}{100,000}}
	 	 				 = \mathbf{0.000189966}
	    \end{align*}
		\item The probability a widget is not defective and tests positive is:
		\begin{align*}
		P(R \cap \lnot D) = P(R \vert \lnot D)P(\lnot D) 	&= (.05)\frac{99,999}{100,000} \\
															&= 0.0499995
		\end{align*}
		The probability a widget is defective and does not test positive is:
		\begin{align*}
		P(\lnot R \cap D) = P(\lnot R \vert  D)P(D) 	&= \frac{.05}{100,000} \\
														&= 0.0000005
		\end{align*}
		Therefore 499995 non-defective widgets are thrown out and 5 defective widgets are shipped per year.
		\end{enumerate}
		\item For KNN classification with $n$ data points...
		\begin{enumerate}
			\item When $k = n$, all data points are considered for any prediction, and therefore any prediction will simply yield the prior probability of the training data. As k approaches 1, fewer training points are taken in to consideration for a prediction, therfore, as $k \rightarrow n$ the mean squared prediction error decreases. When $k = 1$, the classifier considers only $x_i$ for prediction, and thus the prediction error becomes 0.
			\item Previously, when training and testing data were the same set of points, the 0-1 prediction error was 0 for for $k=1$, and approaches $\frac{1}{2}$ as k approaches $n$. When testing on the \textit{held-out half}, the error is low to when $k=1$ (relative to how much the 0-1 classes \text{overlap}) and gradually increases to $\frac{1}{2}$ as $k$ approaches $n$. In the image below, the dotted line is training kNN with all of the training data as in (a) and the solid line is when the training data is split.
			\begin{center}
			\includegraphics[width=7cm]{images/sketch_8b.png}
			\end{center}
			\item If we have a data set containing $n$ datapoints, the relative computational cost $C_i$, of performing cross validation, as a function of $i$ folds can be expressed as:
			
			\begin{equation*}
			 C = \text{(number of folds)} \text{(data points in testing set)} \text{(distance calculation for each point in the training set)}
			\end{equation*}
			
			\begin{equation*}
			 C_i = i \left( \frac{n}{i} \right)\left( \frac{n(i-1)}{i} \right) =  \frac{n^2(i-1)}{i}
			\end{equation*}
			
			\newpage
			
			Extending our conclusion from (b), candidates for $k$ should be tested with an adjustment proportional to $\frac{i-1}{i}$ (specifically - adjust k according to the change in density that occurs when removing $\frac{1}{i}$th of the samples from the training data - see (e); one may also consider the dimensionality, of the feature vector, $x$). \\
			
			Choosing $i$ to be large (say 10), will mitigate the impact of density reduction, however the larger $i$ is more computationally expensive($i \rightarrow \infty, C_i \rightarrow n^2$). As such, if performance is a concern, then $k=3$ is a reasonable choice for at this point, $C=\frac{2}{3} n^2$. Once $k=5$, however, $C=\frac{4}{5} n^2$, and at $k=10$, $C=\frac{9}{10} n^2$. At this point, any sadvings in cost are negligible.
						
			\item If $k$ is large, for some $x$ it may be beneficial to weigh votes from a neighbor $N_k(x)$ using some function $w$ that decreases as the distance $d$, from $x$ increases. E.g.:
			\begin{equation*}
				w_{k}(x,N_k(x)) = \frac{1}{1 + d(x,N_k(x) )}
			\end{equation*}
			
			\item As the dimension of a model increases, the \textit{closeness} of a point to it's neighbors decreases. That is, the the size of the neighborhood about a point does not scale linearly with the part of each each input dimension needed to cover that neighborhood. If we consider the unit space in 1, 2, and 3 dimensions and create subspaces that are half of the input unit spaces for each dimension, then the respective volume is $\frac{1}{2}$, $\frac{1}{4}$, and $\frac{1}{8}$. The inverse is also true; to maintain a fixed fraction of the total space, a large interval of each dimension must be considered. Additionally, the stability of the KNN algorithm relies upon a dense training sample. For a fixed number of points in dimension $N$, increasing the dimension by $p'$ requires a an increase in the number of data points on the order of to $N^\frac{1}{p'}$.
		\end{enumerate}
		
	
\end{enumerate}

\end{document}
 